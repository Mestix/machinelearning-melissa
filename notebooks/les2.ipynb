{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os\n",
    "sys.path.append(os.path.abspath('../models'))\n",
    "sys.path.append(os.path.abspath('../dev'))\n",
    "\n",
    "import torch\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from datetime import datetime as _datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# altijd runnen\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "preprocessor = BasePreprocessor()\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=batchsize, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()\n",
    "\n",
    "accuracy = metrics.Accuracy()\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkWithDropout(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [512, 256]\n",
    "epochs = 20\n",
    "results = []\n",
    "logdir = \"modellogs\"\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=epochs,\n",
    "    metrics=[accuracy],\n",
    "    logdir=logdir,\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "model = NeuralNetworkWithDropout(num_classes=10, units1=units[0], units2=units[1])\n",
    "\n",
    "settings.logdir = f\"{logdir}/DROPOUT/u{units[0]}_u{units[1]}_e{epochs}\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_func,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkWithDropoutNormalization(nn.Module):\n",
    "    def __init__(self, num_classes: int, units1: int, units2: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, units1),\n",
    "            nn.BatchNorm1d(units1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(units1, units2),\n",
    "            nn.BatchNorm1d(units2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(units2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [704, 288]\n",
    "epochs = 25\n",
    "results = []\n",
    "logdir = \"modellogs\"\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=epochs,\n",
    "    metrics=[accuracy],\n",
    "    logdir=logdir,\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "model = NeuralNetworkWithDropoutNormalization(num_classes=10, units1=units[0], units2=units[1], dropout = 0.2)\n",
    "\n",
    "settings.logdir = f\"{logdir}/DROPOUT/u{units[0]}_u{units[1]}_e{epochs}\"\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    settings=settings,\n",
    "    loss_fn=loss_func,\n",
    "    optimizer=optim.Adam,\n",
    "    traindataloader=trainstreamer,\n",
    "    validdataloader=validstreamer,\n",
    "    scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "trainer.loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "experiment_path = \"mlflow_test\"\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope\n",
    "from pathlib import Path\n",
    "\n",
    "modeldir = Path(\"models\").resolve()\n",
    "\n",
    "if not modeldir.exists():\n",
    "    modeldir.mkdir()\n",
    "    print(f\"Created {modeldir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=25,\n",
    "    metrics=[accuracy],\n",
    "    logdir=modeldir,\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML],\n",
    ")\n",
    "\n",
    "# Define the objective function for hyperparameter optimization\n",
    "def objective(params):\n",
    "    with mlflow.start_run():\n",
    "        mlflow.set_tag(\"model\", \"dense-net\")\n",
    "        mlflow.set_tag(\"dev\", \"Melissa\")\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_param(\"batchsize\", f\"{batchsize}\")\n",
    "\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam\n",
    "\n",
    "        model = NeuralNetworkWithDropoutNormalization(num_classes=10, units1=params[\"units1\"], units2=params[\"units2\"], dropout=params[\"dropout\"])\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        )\n",
    "        trainer.loop()\n",
    "\n",
    "        tag = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        modelpath = modeldir / (tag + \"_model.pt\")\n",
    "        torch.save(model, modelpath)\n",
    "\n",
    "        mlflow.log_artifact(local_path=modelpath, artifact_path=\"pytorch_models\")\n",
    "        return {'loss' : trainer.test_loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = {\n",
    "    'units1': scope.int(hp.quniform('units1', 32, 1028, 32)),\n",
    "    'units2': scope.int(hp.quniform('units2', 32, 1028, 32)),\n",
    "    'dropout': hp.uniform('dropout', 0.0, 0.5)}\n",
    "\n",
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=3,\n",
    "    trials=Trials()\n",
    ")\n",
    "\n",
    "best_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CNN import CNNConfig, CNNblocks \n",
    "\n",
    "# Metric en instellingen\n",
    "accuracy = metrics.Accuracy()\n",
    "settings = TrainerSettings(\n",
    "    epochs=5,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"modellogs/cnn_mlflow\",\n",
    "    train_steps=100,\n",
    "    valid_steps=100,\n",
    "    reporttypes=[ReportTypes.MLFLOW, ReportTypes.TOML]\n",
    ")\n",
    "\n",
    "def objective(params):\n",
    "    config = CNNConfig(\n",
    "        hidden=params[\"hidden\"],\n",
    "        num_layers=params[\"num_layers\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        batchsize=params[\"batchsize\"]\n",
    "    )\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_params(config.__dict__)\n",
    "        mlflow.set_tag(\"model\", \"CNNblocks\")\n",
    "\n",
    "        streamers = fashionfactory.create_datastreamer(batchsize=config.batchsize, preprocessor=preprocessor)\n",
    "        trainstreamer = streamers[\"train\"].stream()\n",
    "        validstreamer = streamers[\"valid\"].stream()\n",
    "\n",
    "        model = CNNblocks(config)\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=nn.CrossEntropyLoss(),\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau,\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        )\n",
    "\n",
    "        trainer.loop()\n",
    "\n",
    "        timestamp = _datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "        model_path = f\"modellogs/cnn_mlflow/model_{timestamp}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        mlflow.log_artifact(model_path, artifact_path=\"pytorch_models\")\n",
    "\n",
    "        return {'loss': trainer.test_loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoekruimte definieren\n",
    "search_space = {\n",
    "    'hidden': scope.int(hp.quniform('hidden', 32, 256, 32)),\n",
    "    'num_layers': scope.int(hp.quniform('num_layers', 2, 6, 1)),\n",
    "    'dropout': hp.uniform('dropout', 0.1, 0.5),\n",
    "    'batchsize': scope.int(hp.quniform('batchsize', 32, 128, 32)),\n",
    "}\n",
    "\n",
    "# Hyperparameter optimalisatie uitvoeren\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=10,\n",
    "    trials=trials\n",
    ")\n",
    "\n",
    "print(\"Beste configuratie:\", best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import toml\n",
    "import pandas as pd\n",
    "\n",
    "log_dir = \"modellogs/cnn_mlflow\"\n",
    "runs = []\n",
    "\n",
    "for root, _, files in os.walk(log_dir):\n",
    "    for file in files:\n",
    "        if file == \"model.toml\":\n",
    "            path = os.path.join(root, file)\n",
    "            try:\n",
    "                data = toml.load(path)\n",
    "                config = data.get(\"model\", {}).get(\"config\", {})\n",
    "                runs.append({\n",
    "                    \"path\": path,\n",
    "                    \"hidden\": config.get(\"hidden\"),\n",
    "                    \"dropout\": config.get(\"dropout\"),\n",
    "                    \"batchsize\": config.get(\"batchsize\"),\n",
    "                    \"num_layers\": config.get(\"num_layers\"),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Kon {path} niet inlezen: {e}\")\n",
    "\n",
    "df = pd.DataFrame(runs)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.pairplot(df, hue=\"accuracy\", palette=\"coolwarm\")\n",
    "\n",
    "plt.suptitle(\"Hyperparameter relaties\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
